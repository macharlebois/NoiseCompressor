import easygui
import os
import pandas as pd
import polars as pl
from pointcloud import (
    writer as writer,
    validator as valid,
    skeletonizer as skeletonizer,
)
from pointcloud.utility import (
    select_threshold,
    search_file,
    set_parameters,
    colored,
)
from step2_COMPRESSOR import compress_cloud

import io
from plyfile import PlyData
import urllib.request



if __name__ == "__main__":
    # SELECT THRESHOLD
    threshold = select_threshold(utilisation="compressor")
    while threshold == 'Fraternity index':
        print(" ")
        print(colored("Demo available for Skeleton index only", 'C3'))
        threshold = select_threshold(utilisation="compressor")
    if threshold is None:
        print(" ")
        print(colored("Process canceled by user...", 'C3'))
        exit()


    # IMPORT CLOUD FILE
    cloud_file = ("https://github.com/macharlebois/Sample_Data/raw/refs/heads/main/"
                  "noise_compressor/demo_dataset/individual_stem_files/22729.ply")
    with urllib.request.urlopen(cloud_file) as response:
        ply_data = PlyData.read(io.BytesIO(response.read()))
    vertex_data = ply_data["vertex"].data
    orig_cloud = pd.DataFrame(vertex_data)

    orig_cloud = valid.validate_columns(orig_cloud, threshold)
    if orig_cloud is None:
        print(" ")
        print(colored("Process canceled by user...", 'red'))
        exit()

    # 2-CHOOSE OUTPUT DIRECTORY
    work_directory = easygui.diropenbox(title="OUTPUT DIRECTORY", msg="Select your output directory")
    if work_directory is None:
        print(" ")
        print(colored("Process canceled by user...", 'C3'))
        exit()
    os.chdir(work_directory)

    orig_file = work_directory + '\\' + 'demo_untreated.ply'
    writer.write_ply(orig_file, orig_cloud)

    # Generate a skeleton (or use an existing one)
    generate_skeleton = search_file("skeleton")  # ANSWER 'YES' FOR DEMO
    while generate_skeleton == "No":
        print(" ")
        print(colored("SKELETON REQUIRED FOR DEMO", 'C3'))
        generate_skeleton = search_file("skeleton")
    if generate_skeleton is None:
        print(" ")
        print(colored("Process canceled by user...", 'C3'))
        exit()

    skeleton_file = work_directory + '\\' + 'demo_skeleton.csv'

    # Set compression parameters
    # The value of each parameter is defined according to the result of the optimizer
    # and must be manually adjusted by the user when asked by this script.
    # The optimized parameter values can be found on the plot generated by the optimizer.
    param_comp = dict()
    try:
        (
            param_comp["m1"],
            param_comp["m2"],
            param_comp["b"],
            param_comp["SI_threshold"],
        ) = set_parameters(usage="compression", threshold=threshold)
        print(" ")
        print("The compression parameters have been defined as :")
        print("m1 =", param_comp["m1"])
        print("m2 =", param_comp["m2"])
        print("b =", param_comp["b"])
        print("SI_threshold =", param_comp["SI_threshold"])
    except KeyError:
        print(" ")
        print(colored("The compression process could not be completed without the required parameters.", 'red'))
        print("Please try again.")
        exit()

    # Output file save as *.ply
    compressed_file = work_directory + '\\' + 'demo_compressed.csv'

    # Cloud data validation and identification depending on the compression threshold
    cloud_data = valid.validate_data(orig_cloud, "compressor", threshold)

    # 4-SKELETONIZATION
    # VALIDATION to ensure that all skeleton lines match the cloud lines
    data, cloud_data = valid.validate_data(cloud_data, "skeleton")

    # GENERATE SKELETON
    print(" ")
    print(colored("Generating skeleton..."))
    print("This may take a moment. Please wait.")
    param_sk = {
        "voxel_size": 0.01,
        "search_radius": 0.1,
        "max_relocation_dist": 0.21,
    }
    cloud_data = pl.from_pandas(cloud_data)
    skeleton = skeletonizer.generate_skeleton(cloud_data, param_sk)
    skeleton = skeleton.to_pandas()

    # ADD missing columns to the skeleton file
    cols_to_add = [col for col in data.columns if col not in skeleton.columns or col == "line_id"]
    skeleton_data = pd.merge(data[cols_to_add], skeleton, on="line_id", how="left")
    writer.write_csv(skeleton_file, skeleton_data)
    print(" ")
    print("Skeletonization completed" + colored("successfully", "C14") + ".")


    # 5-COMPRESS POINT CLOUD
    print(" ")
    print(colored("Working on point cloud compression..."))

    skeleton_data = pl.from_pandas(skeleton_data)
    if isinstance(cloud_data, pd.DataFrame):
        cloud_data = pl.from_pandas(cloud_data)

    result = compress_cloud(cloud_data, skeleton_data, param_comp, threshold)
    result = result.to_pandas()

    # ADD missing columns to the compressed file
    cols_to_add = [col for col in data.columns if col not in result.columns or col == "line_id"]
    result_data = pd.merge(result, data[cols_to_add], on="line_id", how="left")

    # Save compressed cloud
    print(" ")
    print(colored("Saving results..."))
    writer.write_ply(compressed_file, result_data)

    print(" ")
    print("Compression with '%s threshold': " % threshold + colored("COMPLETED", 'C14'))
    print(" ")
    print("RESULTS SAVED IN : " + colored(work_directory, "C14"))
